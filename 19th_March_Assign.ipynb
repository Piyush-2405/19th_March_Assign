{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d31554-2311-49d1-a5fe-b35bdaea2198",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6402-8d2e-4860-9678-d1b62ecbe19a",
   "metadata": {},
   "source": [
    "\n",
    "Min-Max scaling, also known as feature scaling or normalization, is a data preprocessing technique used in machine learning to rescale numerical features within a specific range, typically between 0 and 1. This transformation is applied to the entire dataset, ensuring that all the features have the same scale, which can be crucial for some machine learning algorithms. Min-Max scaling is particularly useful when the features have different units or scales, as it can help prevent certain features from dominating the learning process due to their larger magnitude.\n",
    "\n",
    "The formula for Min-Max scaling is as follows for each feature:\n",
    "\n",
    "\n",
    "Xscaled= (X−Xmin)/(Xmax−Xmin)\n",
    "\n",
    "Where:\n",
    "\n",
    "Xscaled is the scaled value of the feature.\n",
    "\n",
    "X is the original value of the feature.\n",
    "\n",
    "Xmin is the minimum value of the feature in the dataset.\n",
    "\n",
    "Xmax is the maximum value of the feature in the dataset.\n",
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset with a single numerical feature, \"Age,\" which has values ranging from 18 to 65. You want to apply Min-Max scaling to this feature to rescale it between 0 and 1.\n",
    "\n",
    "1.Find the minimum and maximum values of the \"Age\" feature in your dataset:\n",
    "\n",
    "Xmin=18\n",
    "\n",
    "Xmax=65\n",
    "\n",
    "2.Choose a data point, let's say \"Age = 30,\" from your dataset and scale it using the Min-Max scaling formula:\n",
    "\n",
    "Xscaled=(30−18) /(65−18)\n",
    "       = 12/47\n",
    "\n",
    "So, the scaled value for \"Age = 30\" after Min-Max scaling is approximately 0.2553.\n",
    "\n",
    "Repeat this process for all the data points in the \"Age\" column to obtain the scaled values for the entire feature.\n",
    "\n",
    "After applying Min-Max scaling, your \"Age\" feature will be transformed to a new feature where all values lie between 0 and 1, making it suitable for use in machine learning algorithms that are sensitive to feature scaling, such as support vector machines (SVM) and neural networks.\n",
    "\n",
    "Keep in mind that Min-Max scaling assumes that your data is approximately uniformly distributed within the specified range, and it might not be suitable for data that has outliers. In such cases, you may want to consider other scaling techniques, like Z-score standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d0d2f-efac-4647-9043-2193dfa83c0d",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11af182-2a2f-4b96-adba-e31be1c5bbaf",
   "metadata": {},
   "source": [
    "\n",
    "The unit vector technique is a feature scaling technique that scales each feature vector to have a unit length. This means that the Euclidean norm of each feature vector is equal to 1. The Euclidean norm is the distance from the origin to the feature vector.\n",
    "\n",
    "To perform unit vector scaling, we can use the following formula:\n",
    "\n",
    "scaled_vector = feature_vector / ||feature_vector||\n",
    "where scaled_vector is the scaled feature vector, feature_vector is the original feature vector, and ||feature_vector|| is the Euclidean norm of the feature vector.\n",
    "\n",
    "Min-Max scaling is another feature scaling technique that scales each feature vector to a range of [0, 1]. This is done by subtracting the minimum value of each feature from all the values in the feature, and then dividing by the range of the feature.\n",
    "\n",
    "The main difference between unit vector scaling and Min-Max scaling is that unit vector scaling preserves the direction of the feature vectors, while Min-Max scaling does not. This means that unit vector scaling is more suitable for algorithms that are sensitive to the direction of the feature vectors, such as principal component analysis (PCA).\n",
    "\n",
    "An example of the application of unit vector scaling is in the field of image processing. In image processing, it is common to use unit vector scaling to normalize the pixel values of an image. This is done to ensure that all the pixel values have the same range, which can help to improve the performance of image processing algorithms.\n",
    "\n",
    "For example, let's say we have an image with pixel values between 0 and 255. We can use unit vector scaling to normalize the pixel values to a range of [0, 1] by subtracting 0 from all the pixel values, and then dividing by 255.\n",
    "\n",
    "This would result in all the pixel values being between 0 and 1, which would make it easier for image processing algorithms to work with the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca508b5-5b95-44d2-8937-968998b98854",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e1b68-63e5-4e6b-9dba-11f47be29667",
   "metadata": {},
   "source": [
    "PCA, which stands for Principal Component Analysis, is a dimensionality reduction technique used in the field of machine learning and statistics. Its primary goal is to reduce the number of features (or dimensions) in a dataset while preserving as much of the relevant information as possible. PCA achieves this by transforming the original features into a new set of orthogonal (uncorrelated) features called principal components. These principal components are linear combinations of the original features and are sorted in such a way that the first principal component retains the most variance in the data, the second retains the second-most variance, and so on.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "1.Standardization: If the features in your dataset have different scales or units, it's common to standardize them (subtract the mean and divide by the standard deviation) to ensure they have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2.Covariance Matrix: PCA calculates the covariance matrix of the standardized data. The covariance matrix shows how different features relate to each other.\n",
    "\n",
    "3.Eigenvalue Decomposition: PCA then performs eigenvalue decomposition on the covariance matrix to find the eigenvalues and eigenvectors. The eigenvectors represent the directions of maximum variance in the data, and the eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "4.Selection of Principal Components: The principal components are chosen based on the eigenvalues. The first principal component (PC1) corresponds to the eigenvector with the highest eigenvalue, the second principal component (PC2) corresponds to the eigenvector with the second-highest eigenvalue, and so on. Typically, you select a subset of these principal components to reduce the dimensionality.\n",
    "\n",
    "5.Projection: Finally, PCA projects the original data onto the selected principal components to obtain a lower-dimensional representation of the data.\n",
    "\n",
    "Here's a simplified example to illustrate PCA:\n",
    "\n",
    "Suppose you have a dataset with two features, \"Height\" (in centimeters) and \"Weight\" (in kilograms), and you want to reduce it to one dimension using PCA.\n",
    "\n",
    "1.Standardize the data if necessary.\n",
    "\n",
    "2.Calculate the covariance matrix:\n",
    "\n",
    "[Cov(Height, Height)    Cov(Height, Weight)\n",
    " Cov(Weight, Height)    Cov(Weight, Weight)]\n",
    "\n",
    "3.Perform eigenvalue decomposition on the covariance matrix to obtain eigenvectors and eigenvalues.\n",
    "\n",
    "4.The first principal component (PC1) corresponds to the eigenvector with the highest eigenvalue. Let's say PC1 is primarily a combination of \"Height\" and \"Weight\" with a weight of [0.6, 0.8].\n",
    "\n",
    "5.Project your data onto PC1:\n",
    "\n",
    "PCA_Data=Height⋅0.6+Weight⋅0.8\n",
    "\n",
    "Now, you have reduced your two-dimensional data to one dimension while preserving the most significant information. This can be particularly useful for visualization, simplifying models, and speeding up computation in cases where many features are not essential for the task at hand.\n",
    "\n",
    "PCA allows you to choose how many principal components to retain, trading off dimensionality reduction with information retention. Typically, you might select a subset of principal components that explain a high percentage (e.g., 95%) of the variance in the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bfe7c-672c-495b-96f4-52c26c483c58",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c84f8-47db-4ed0-8e4a-f92104ba7db5",
   "metadata": {},
   "source": [
    "\n",
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts in the context of dimensionality reduction and data analysis. PCA can be used as a technique for feature extraction, and it serves as a method to transform high-dimensional data into a lower-dimensional representation while retaining the most important information.\n",
    "\n",
    "Here's how PCA relates to feature extraction and how it can be used for this purpose:\n",
    "\n",
    "1. Feature Extraction with PCA:\n",
    "\n",
    "Original Data: In feature extraction, you start with a dataset that has a high number of features (high dimensionality).\n",
    "\n",
    "Dimensionality Reduction: The goal is to reduce the dimensionality of the dataset by selecting a subset of new features (derived from the original features) that capture the most relevant information while discarding redundant or less informative features.\n",
    "\n",
    "PCA Transformation: PCA achieves feature extraction by transforming the original features into a set of orthogonal principal components. These principal components are linear combinations of the original features and are ranked by the amount of variance they explain in the data.\n",
    "\n",
    "Feature Selection: To perform feature extraction using PCA, you typically select a subset of the principal components based on their corresponding eigenvalues. Principal components with higher eigenvalues capture more variance in the data and are considered more important. These selected principal components become your new features.\n",
    "\n",
    "2. Example: Feature Extraction with PCA:\n",
    "\n",
    "Let's consider an example where you have a dataset with five numerical features, but you want to reduce it to only two features using PCA for feature extraction.\n",
    "\n",
    "1.Original Data: You start with a dataset that looks like this:\n",
    "\n",
    "Feature1  Feature2  Feature3  Feature4  Feature5\n",
    "2.5       1.0       0.5       2.2       3.8\n",
    "3.2       1.2       0.8       2.8       4.1\n",
    "...       ...       ...       ...       ...\n",
    "\n",
    "2.PCA Transformation: You apply PCA to this dataset, and it calculates the principal components. Let's say the first two principal components, PC1 and PC2, capture the most variance in the data. These become your new features.\n",
    "\n",
    "3.Feature Extraction: The new dataset, after feature extraction with PCA, looks like this:\n",
    "\n",
    "PC1       PC2\n",
    "-0.63     0.10\n",
    "-0.52     0.08\n",
    "...       ...\n",
    "\n",
    "Now, you have successfully extracted two new features, PC1 and PC2, from the original five features. These new features capture the most important information in the data, and you can use them for further analysis or modeling. By reducing the dimensionality, you may have simplified your dataset while preserving its essential characteristics.\n",
    "\n",
    "In summary, PCA is a powerful tool for feature extraction that allows you to transform high-dimensional data into a lower-dimensional representation by selecting a subset of principal components that capture the most variance in the data. This can be especially valuable when dealing with datasets with a large number of features or when preparing data for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c105a-be5b-4657-a536-25309be8a4c3",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51924c5c-4ae1-41ad-9164-48c0c66c5bf2",
   "metadata": {},
   "source": [
    "Min-Max scaling is a common preprocessing technique used in machine learning to normalize or standardize numerical features within a specific range, typically between 0 and 1. This is particularly useful when you have features with different scales and you want to ensure that they contribute equally to the model's learning process. In the context of building a recommendation system for a food delivery service with features like price, rating, and delivery time, you can use Min-Max scaling as follows:\n",
    "\n",
    "1.Understand the Data: First, you need to have a clear understanding of your dataset and the range of values for each feature. In your case, you mentioned three features: price, rating, and delivery time. Determine the minimum and maximum values for each of these features.\n",
    "\n",
    "2.Apply Min-Max Scaling: To scale each feature to a common range, typically between 0 and 1, you can use the Min-Max scaling formula for each feature:\n",
    "\n",
    "For a given feature x:\n",
    "\n",
    "xscaled = (x−xmin)/(xmax−xmin)\n",
    "\n",
    "Where:\n",
    "\n",
    "x is the original value of the feature.\n",
    "\n",
    "xscaled is the scaled value of the feature.\n",
    "\n",
    "xmin is the minimum value of the feature in the dataset.\n",
    "\n",
    "xmax is the maximum value of the feature in the dataset.\n",
    "\n",
    "3.Implement Min-Max Scaling: You can implement this scaling process using a library like scikit-learn in Python. \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on your dataset and transform the data\n",
    "scaled_data = scaler.fit_transform(your_data[['price', 'rating', 'delivery_time']])\n",
    "\n",
    "4.Updated Data: After applying Min-Max scaling, you will have a new dataset where the values of the 'price,' 'rating,' and 'delivery_time' features are scaled between 0 and 1.\n",
    "\n",
    "5.Benefits of Min-Max Scaling:\n",
    "\n",
    "Min-Max scaling ensures that all features are on the same scale, preventing features with larger numerical ranges from dominating the learning process.\n",
    "\n",
    "It can help improve the convergence of machine learning algorithms, especially those that rely on distance metrics or gradient-based optimization.\n",
    "\n",
    "Scaling features to a common range makes it easier to compare and interpret the importance of each feature in your recommendation system.\n",
    "Use in Recommendation System: You can now use this scaled data as input features for building your recommendation system. Depending on the specific algorithm you choose (e.g., collaborative filtering, content-based filtering, or hybrid methods), you can use these scaled features along with other relevant information to make food recommendations to users.\n",
    "\n",
    "Remember that while Min-Max scaling is a common and effective preprocessing technique, it may not be suitable for all types of data or algorithms. Always consider the characteristics of your data and the requirements of your recommendation system when deciding on preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecadbe0b-ced5-4966-9b2e-dcc8d9a190f9",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6a7ab-e4d5-4985-84ae-4208eb5de2a5",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning to reduce the complexity of high-dimensional datasets while retaining as much relevant information as possible. When building a model to predict stock prices using a dataset with numerous features, including company financial data and market trends, PCA can be a valuable tool for dimensionality reduction. Here's how you can use PCA for this purpose:\n",
    "\n",
    "1.Data Preparation:\n",
    "\n",
    "Start by cleaning and preprocessing your dataset. Handle missing values, outliers, and any other data quality issues.\n",
    "\n",
    "Ensure that the features you want to include in your PCA analysis are numeric and on a similar scale. You may need to normalize or standardize the data if necessary.\n",
    "\n",
    "2.Standardization (Optional):\n",
    "\n",
    "Depending on the scale and units of measurement of your features, it's often a good practice to standardize the data to have zero mean and unit variance. This step can help ensure that features with larger scales do not dominate the PCA process.\n",
    "\n",
    "3.Perform PCA:\n",
    "\n",
    "Apply PCA to your dataset. Most machine learning libraries, such as scikit-learn in Python, provide tools for PCA.\n",
    "\n",
    "Specify the number of principal components (PCs) you want to retain. This choice depends on your desired level of dimensionality reduction and the amount of variance you want to explain.\n",
    "\n",
    "4.Determine the Number of Components:\n",
    "\n",
    "To decide how many principal components to keep, you can examine the explained variance ratio. It tells you the proportion of the total variance in the original data that is explained by each principal component.\n",
    "\n",
    "A common approach is to choose the number of components that collectively explain a sufficiently high percentage of the total variance, such as 95% or 99%. You can visualize this by plotting the cumulative explained variance.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance and fit it to your standardized data\n",
    "pca = PCA()\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_explained_variance)\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.show()\n",
    "\n",
    "5.Select the Number of Components:\n",
    "\n",
    "Based on the cumulative explained variance plot, choose the number of principal components that capture the desired proportion of variance.\n",
    "\n",
    "6.Transform Data:\n",
    "\n",
    "Apply PCA transformation to your dataset using the selected number of principal components.\n",
    "\n",
    "# Transform your data with the selected number of components\n",
    "num_components = 10  # Replace with your chosen number\n",
    "pca = PCA(n_components=num_components)\n",
    "reduced_data = pca.transform(standardized_data)\n",
    "\n",
    "7.Model Building:\n",
    "\n",
    "Use the reduced dataset, which now has fewer dimensions, as input to your stock price prediction model.\n",
    "\n",
    "You can choose a regression algorithm like linear regression, time series models, or more advanced machine learning techniques depending on the nature of your problem.\n",
    "\n",
    "Using PCA for dimensionality reduction in your stock price prediction project can help you reduce the risk of overfitting, improve model training time, and potentially reveal the most significant underlying patterns in your financial and market data. However, be mindful of the trade-off between dimensionality reduction and the loss of information when selecting the number of principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a421393-fd2f-45eb-95dc-e8c8d5d58092",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fdfd61-f202-4033-bb1d-c22a50c2041a",
   "metadata": {},
   "source": [
    "Min-Max scaling is a technique used to transform data into a specific range, typically between 0 and 1. In this case, you want to scale the values in the dataset to a range of -1 to 1. Here's how you can perform Min-Max scaling to achieve that:\n",
    "\n",
    "Understand the Data:\n",
    "\n",
    "Your dataset contains the following values: [1, 5, 10, 15, 20].\n",
    "You want to transform these values to a range of -1 to 1.\n",
    "Calculate Min and Max:\n",
    "\n",
    "Calculate the minimum and maximum values in the original dataset.\n",
    "\n",
    "Min = 1\n",
    "Max = 20\n",
    "\n",
    "Perform Min-Max Scaling:\n",
    "\n",
    "Use the Min-Max scaling formula to scale each value to the desired range (-1 to 1):\n",
    "\n",
    "x_scaled = (x - Min) / (Max - Min) * (new_max - new_min) + new_min\n",
    "\n",
    "Substitute the values into the formula:\n",
    "\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "For x = 1:\n",
    "x_scaled = (1 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0.0625 * 2 - 1 = -0.875\n",
    "\n",
    "For x = 5:\n",
    "x_scaled = (5 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0.25 * 2 - 1 = -0.5\n",
    "\n",
    "For x = 10:\n",
    "x_scaled = (10 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0.4375 * 2 - 1 = -0.125\n",
    "\n",
    "For x = 15:\n",
    "x_scaled = (15 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0.625 * 2 - 1 = 0.25\n",
    "\n",
    "For x = 20:\n",
    "x_scaled = (20 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 1 * 2 - 1 = 1\n",
    "\n",
    "Scaled Dataset:\n",
    "\n",
    "After performing Min-Max scaling, the scaled dataset in the range of -1 to 1 is as follows:\n",
    "\n",
    "[-0.875, -0.5, -0.125, 0.25, 1]\n",
    "\n",
    "Now, you have successfully transformed the original dataset values to the desired range of -1 to 1 using Min-Max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4572a-418c-4638-9bba-39a758243519",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba824593-f6b1-4be1-934c-8da74467d73a",
   "metadata": {},
   "source": [
    "Performing feature extraction using Principal Component Analysis (PCA) involves reducing the dimensionality of your dataset while retaining as much variance as possible. The number of principal components to retain depends on several factors, including the amount of variance you want to preserve and the specific goals of your analysis. Here's how you can decide how many principal components to retain in your dataset containing the features: height, weight, age, gender, and blood pressure:\n",
    "\n",
    "1.Data Preprocessing:\n",
    "\n",
    "Start by preprocessing your data. This may include handling missing values, standardizing or normalizing features, and encoding categorical variables like gender.\n",
    "\n",
    "2.Standardization:\n",
    "\n",
    "Standardize your numeric features (height, weight, age, and blood pressure) to have a mean of 0 and a standard deviation of 1. Standardization is important in PCA to ensure that features with larger scales do not dominate the principal components.\n",
    "\n",
    "3.Perform PCA:\n",
    "\n",
    "Apply PCA to your standardized dataset. Most PCA implementations will allow you to specify the number of components to retain.\n",
    "\n",
    "4.Examine Explained Variance:\n",
    "\n",
    "Look at the explained variance ratio for each principal component. The explained variance ratio tells you the proportion of the total variance in the data that each principal component explains.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA on the standardized data\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "5.Determine Number of Components to Retain:\n",
    "\n",
    "One common approach is to choose the number of principal components that collectively explain a sufficiently high percentage of the total variance. A commonly used threshold is 95% or 99% of the total variance.\n",
    "You can calculate the cumulative explained variance to help you make this decision.\n",
    "\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Find the number of components that explain 95% of the variance\n",
    "num_components_to_retain = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "6.Interpretability vs. Information Loss:\n",
    "\n",
    "Consider the trade-off between retaining fewer components (which may simplify your model and improve interpretability) and retaining more components (which may capture more of the data's variance but make interpretation harder).\n",
    "Also, consider the practical implications of dimensionality reduction. Fewer components can lead to faster model training and reduced computational complexity.\n",
    "\n",
    "7.Final Choice:\n",
    "\n",
    "Based on your analysis and specific goals, choose the number of principal components to retain. For example, you might choose to retain three components if they explain 95% of the variance in your data.\n",
    "The number of principal components to retain is a subjective decision and should align with your project's objectives. If interpretability of the components is essential, you may opt for a smaller number of components. However, if capturing as much variance as possible is crucial, you may choose to retain more components. It's also a good practice to experiment with different numbers of components and evaluate the impact on your downstream tasks, such as modeling or classification, to make an informed decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb00602-d7da-4a45-b506-ab751d0219e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
